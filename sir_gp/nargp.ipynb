{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c2e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a53c1de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([780, 54]) torch.Size([780, 1800]) torch.Size([780, 324])\n",
      "torch.Size([150, 255]) torch.Size([150, 8500]) torch.Size([150, 7225])\n",
      "torch.Size([780, 1800]) torch.Size([780, 1800])\n",
      "torch.Size([150, 8500]) torch.Size([150, 8500])\n",
      "torch.Size([1560, 8500]) torch.Size([1560, 8500])\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(dataset_dir):\n",
    "    data = {}\n",
    "\n",
    "    for category in ['train_l1', 'train_l2', 'val_l1', 'val_l2', 'test_l1', 'test_l2']:\n",
    "        cat_data = np.load(os.path.join(dataset_dir, category + '.npz'))\n",
    "        data['x_' + category] = cat_data['x']\n",
    "        data['y_' + category] = cat_data['y']\n",
    "        data['graph_' + category] = cat_data['graph']\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = load_dataset('../nargp_data/')\n",
    "\n",
    "\n",
    "x_train_l = data['x_train_l1']\n",
    "x_train_l = x_train_l.reshape(x_train_l.shape[0],-1)\n",
    "y_train_l = np.log(data['y_train_l1']+10.) \n",
    "y_train_l = y_train_l.reshape(y_train_l.shape[0],-1)\n",
    "graph_train_l = data['graph_train_l1']\n",
    "graph_train_l = graph_train_l.reshape(graph_train_l.shape[0],-1)\n",
    "x_train_h = data['x_train_l2']\n",
    "x_train_h = x_train_h.reshape(x_train_h.shape[0],-1)\n",
    "y_train_h = np.log(data['y_train_l2']+10.) \n",
    "y_train_h = y_train_h.reshape(y_train_h.shape[0],-1)\n",
    "graph_train_h = data['graph_train_l2']\n",
    "graph_train_h = graph_train_h.reshape(graph_train_h.shape[0],-1)\n",
    "\n",
    "x_val_l = data['x_val_l1']\n",
    "x_val_l = x_val_l.reshape(x_val_l.shape[0],-1)\n",
    "y_val_l = np.log(data['y_val_l1']+10.)\n",
    "y_val_l = y_val_l.reshape(y_val_l.shape[0],-1)\n",
    "graph_val_l = data['graph_val_l1']\n",
    "graph_val_l = graph_val_l.reshape(graph_val_l.shape[0],-1)\n",
    "x_val_h = data['x_val_l2']\n",
    "x_val_h = x_val_h.reshape(x_val_h.shape[0],-1)\n",
    "y_val_h = np.log(data['y_val_l2']+10.)\n",
    "y_val_h = y_val_h.reshape(y_val_h.shape[0],-1)\n",
    "graph_val_h = data['graph_val_l2']\n",
    "graph_val_h = graph_val_h.reshape(graph_val_h.shape[0],-1)\n",
    "\n",
    "x_test_l = data['x_test_l1']\n",
    "x_test_l = x_test_l.reshape(x_test_l.shape[0],-1)\n",
    "y_test_l = np.log(data['y_test_l1']+10.)\n",
    "y_test_l = y_test_l.reshape(y_test_l.shape[0],-1)\n",
    "graph_test_l = data['graph_test_l1']\n",
    "graph_test_l = graph_test_l.reshape(graph_test_l.shape[0],-1)\n",
    "x_test_h = data['x_test_l2']\n",
    "x_test_h = x_test_h.reshape(x_test_h.shape[0],-1)\n",
    "y_test_h = np.log(data['y_test_l2']+10.)\n",
    "y_test_h = y_test_h.reshape(y_test_h.shape[0],-1)\n",
    "graph_test_h = data['graph_test_l2']\n",
    "graph_test_h = graph_test_h.reshape(graph_test_h.shape[0],-1)\n",
    "\n",
    "train_x_l = torch.from_numpy(x_train_l)\n",
    "train_y_l = torch.from_numpy(y_train_l)\n",
    "train_graph_l = torch.from_numpy(graph_train_l)\n",
    "\n",
    "train_x_h = torch.from_numpy(x_train_h)\n",
    "train_y_h = torch.from_numpy(y_train_h)\n",
    "train_graph_h = torch.from_numpy(graph_train_h)\n",
    "\n",
    "val_x_l = torch.from_numpy(x_val_l)\n",
    "val_y_l = torch.from_numpy(y_val_l)\n",
    "val_graph_l = torch.from_numpy(graph_val_l)\n",
    "\n",
    "val_x_h = torch.from_numpy(x_val_h)\n",
    "val_y_h = torch.from_numpy(y_val_h)\n",
    "val_graph_h = torch.from_numpy(graph_val_h)\n",
    "\n",
    "test_x_l = torch.from_numpy(x_test_l)\n",
    "test_y_l = torch.from_numpy(y_test_l)\n",
    "test_graph_l = torch.from_numpy(graph_test_l)\n",
    "\n",
    "test_x_h = torch.from_numpy(x_test_h)\n",
    "test_y_h = torch.from_numpy(y_test_h)\n",
    "test_graph_h = torch.from_numpy(graph_test_h)\n",
    "\n",
    "print(train_x_l.shape,train_y_l.shape, train_graph_l.shape)\n",
    "print(train_x_h.shape,train_y_h.shape, train_graph_h.shape)\n",
    "\n",
    "train_x_l = torch.cat([torch.repeat_interleave(train_x_l,28,-1)[:,:1476],train_graph_l],-1).float()\n",
    "val_x_l = torch.cat([torch.repeat_interleave(val_x_l,28,-1)[:,:1476],val_graph_l],-1).float()\n",
    "test_x_l = torch.cat([torch.repeat_interleave(test_x_l,28,-1)[:,:1476],test_graph_l],-1).float()\n",
    "\n",
    "train_x_h = torch.cat([torch.repeat_interleave(train_x_h,5,-1),train_graph_h],-1).float()\n",
    "val_x_h = torch.cat([torch.repeat_interleave(val_x_h,5,-1),val_graph_h],-1).float()\n",
    "test_x_h = torch.cat([torch.repeat_interleave(test_x_h,5,-1),test_graph_h],-1).float()\n",
    "\n",
    "print(train_x_l.shape,train_y_l.shape)\n",
    "print(train_x_h.shape,train_y_h.shape)\n",
    "print(test_x_h.shape,test_y_h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "004e0f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latents = 1\n",
    "num_tasks_l1 = 1800\n",
    "\n",
    "class MultitaskGPModel_l1(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self):\n",
    "        # Let's use a different set of inducing points for each latent function\n",
    "        inducing_points = torch.rand(num_latents, 1, 1800)\n",
    "\n",
    "        # We have to mark the CholeskyVariationalDistribution as batch\n",
    "        # so that we learn a variational distribution for each task\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            inducing_points.size(-2), batch_shape=torch.Size([num_latents])\n",
    "        )\n",
    "\n",
    "        # We have to wrap the VariationalStrategy in a LMCVariationalStrategy\n",
    "        # so that the output will be a MultitaskMultivariateNormal rather than a batch output\n",
    "        variational_strategy = gpytorch.variational.LMCVariationalStrategy(\n",
    "            gpytorch.variational.VariationalStrategy(\n",
    "                self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "            ),\n",
    "            num_tasks=1800,\n",
    "            num_latents=1,\n",
    "            latent_dim= -1\n",
    "        )\n",
    "\n",
    "        super().__init__(variational_strategy)\n",
    "\n",
    "        # The mean and covariance modules should be marked as batch\n",
    "        # so we learn a different set of hyperparameters\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([num_latents]))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([num_latents])),\n",
    "            batch_shape=torch.Size([num_latents])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function should be written as if we were dealing with each output\n",
    "        # dimension in batch\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5595651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latents = 1\n",
    "num_tasks_l2 = 8500 \n",
    "\n",
    "class MultitaskGPModel_l2(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self):\n",
    "        # Let's use a different set of inducing points for each latent function\n",
    "        inducing_points = torch.rand(num_latents, 1, 10300) #8500+1800\n",
    "\n",
    "        # We have to mark the CholeskyVariationalDistribution as batch\n",
    "        # so that we learn a variational distribution for each task\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            inducing_points.size(-2), batch_shape=torch.Size([num_latents])\n",
    "        )\n",
    "\n",
    "        # We have to wrap the VariationalStrategy in a LMCVariationalStrategy\n",
    "        # so that the output will be a MultitaskMultivariateNormal rather than a batch output\n",
    "        variational_strategy = gpytorch.variational.LMCVariationalStrategy(\n",
    "            gpytorch.variational.VariationalStrategy(\n",
    "                self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "            ),\n",
    "            num_tasks= 8500,\n",
    "            num_latents=1,\n",
    "            latent_dim=-1\n",
    "        )\n",
    "\n",
    "        super().__init__(variational_strategy)\n",
    "\n",
    "        # The mean and covariance modules should be marked as batch\n",
    "        # so we learn a different set of hyperparameters\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([num_latents]))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([num_latents]),active_dims=tuple(range(0,1800)))*\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([num_latents]),active_dims=tuple(range(1800,10300)))+\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([num_latents]),active_dims=tuple(range(1800,10300))),\n",
    "            batch_shape=torch.Size([num_latents])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function should be written as if we were dealing with each output\n",
    "        # dimension in batch\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e4fc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2g/wdp07rt57lg1cc7zgtgnr3yh0000gn/T/ipykernel_4602/639063819.py:33: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  epochs_iter = tqdm.tqdm_notebook(range(num_epochs), desc=\"Epoch\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95780387758a4008b287796f35d18015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e89d4e7b6340e3b11de623a1f5390d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d52957036584347a0884833e5ae5e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "num_epochs = 2 if smoke_test else 2000\n",
    "\n",
    "random_seeds = [42,43,44]\n",
    "v_pred_mean_list_l1 = []\n",
    "te_pred_mean_list_l1 = []\n",
    "tr_pred_mean_list_l1 = []\n",
    "saved_model_list_l1 = []\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    torch.manual_seed(random_seeds[i])\n",
    "    np.random.seed(random_seeds[i])\n",
    "    random.seed(random_seeds[i])\n",
    "    \n",
    "    model = MultitaskGPModel_l1()\n",
    "    likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=num_tasks_l1)\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.05)\n",
    "\n",
    "    # Our loss object. We're using the VariationalELBO, which essentially just computes the ELBO\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y_l.size(0))\n",
    "\n",
    "    # We use more CG iterations here because the preconditioner introduced in the NeurIPS paper seems to be less\n",
    "    # effective for VI.\n",
    "    epochs_iter = tqdm.tqdm_notebook(range(num_epochs), desc=\"Epoch\")\n",
    "\n",
    "    min_val_loss = float('inf')\n",
    "    patience = 100\n",
    "    wait = 0\n",
    "\n",
    "    for j in epochs_iter:\n",
    "        # Within each iteration, we will go over each minibatch of data\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x_l)\n",
    "        loss = -mll(output, train_y_l)\n",
    "\n",
    "        epochs_iter.set_postfix(loss=loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # Make predictions\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            val_predictions = likelihood(model(val_x_l))\n",
    "            val_mean = val_predictions.mean.detach().numpy()\n",
    "            val_loss = np.mean(np.abs(val_y_l.numpy() - val_mean))\n",
    "            if val_loss < min_val_loss:\n",
    "                wait = 0\n",
    "                min_val_loss = val_loss\n",
    "                saved_model = model\n",
    "            elif val_loss >= min_val_loss:\n",
    "                wait += 1\n",
    "                if wait == patience:\n",
    "#                     saved_model_list_l1.append(saved_model)\n",
    "#                     te_predictions = likelihood(saved_model(test_x_l))\n",
    "#                     te_mean = te_predictions.mean\n",
    "#                     te_pred_mean_list_l1.append(te_mean)\n",
    "# #                     te_pred_lower, te_pred_upper = te_predictions.confidence_region()\n",
    "# #                     te_pred_std = te_mean - te_pred_lower\n",
    "# #                     te_pred_std_list_l1.append(te_pred_std)\n",
    "                    \n",
    "#                     v_predictions = likelihood(saved_model(val_x_l))\n",
    "#                     v_mean = v_predictions.mean\n",
    "#                     v_pred_mean_list_l1.append(v_mean)\n",
    "# #                     v_pred_lower, v_pred_upper = v_predictions.confidence_region()\n",
    "# #                     v_pred_std = v_mean - v_pred_lower\n",
    "# #                     v_pred_std_list_l1.append(v_pred_std)\n",
    "                    \n",
    "#                     tr_predictions = likelihood(saved_model(train_x_l))\n",
    "#                     tr_mean = tr_predictions.mean\n",
    "#                     tr_pred_mean_list_l1.append(tr_mean)\n",
    "                    break\n",
    "        \n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        saved_model_list_l1.append(saved_model)\n",
    "        te_predictions = likelihood(saved_model(test_x_l))\n",
    "        te_mean = te_predictions.mean\n",
    "        te_pred_mean_list_l1.append(te_mean)\n",
    "#         te_pred_lower, te_pred_upper = te_predictions.confidence_region()\n",
    "#         te_pred_std = te_mean - te_pred_lower\n",
    "#         te_pred_std_list_l1.append(te_pred_std)\n",
    "\n",
    "        v_predictions = likelihood(saved_model(val_x_l))\n",
    "        v_mean = v_predictions.mean\n",
    "        v_pred_mean_list_l1.append(v_mean)\n",
    "#         v_pred_lower, v_pred_upper = v_predictions.confidence_region()\n",
    "#         v_pred_std = v_mean - v_pred_lower\n",
    "#         v_pred_std_list_l1.append(v_pred_std)\n",
    "\n",
    "        tr_predictions = likelihood(saved_model(train_x_l))\n",
    "        tr_mean = tr_predictions.mean\n",
    "        tr_pred_mean_list_l1.append(tr_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cbb6e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2g/wdp07rt57lg1cc7zgtgnr3yh0000gn/T/ipykernel_4602/2963297891.py:32: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  epochs_iter = tqdm.tqdm_notebook(range(num_epochs), desc=\"Epoch\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155643434fd742e989493d265cd71d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2fc83b70797415c9f3015338f719eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1c9176cf3742a49d182999c9cae48c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "num_epochs = 2 if smoke_test else 2000\n",
    "\n",
    "random_seeds = [42,43,44]\n",
    "pred_mean_list_l2 = []\n",
    "pred_std_list_l2 = []\n",
    "saved_model_list_l2 = []\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    torch.manual_seed(random_seeds[i])\n",
    "    np.random.seed(random_seeds[i])\n",
    "    random.seed(random_seeds[i])\n",
    "    \n",
    "    model = MultitaskGPModel_l2()\n",
    "    likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=num_tasks_l2)\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.05)\n",
    "\n",
    "    # Our loss object. We're using the VariationalELBO, which essentially just computes the ELBO\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y_h.size(0))\n",
    "\n",
    "    # We use more CG iterations here because the preconditioner introduced in the NeurIPS paper seems to be less\n",
    "    # effective for VI.\n",
    "    epochs_iter = tqdm.tqdm_notebook(range(num_epochs), desc=\"Epoch\")\n",
    "\n",
    "    min_val_loss = float('inf')\n",
    "    patience = 100\n",
    "    wait = 0\n",
    "\n",
    "    for j in epochs_iter:\n",
    "        # Within each iteration, we will go over each minibatch of data\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_xx = torch.cat((tr_pred_mean_list_l1[i][:train_x_h.shape[0],:], train_x_h),-1)\n",
    "        output = model(train_xx)\n",
    "        loss = -mll(output, train_y_h)\n",
    "\n",
    "        epochs_iter.set_postfix(loss=loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # Make predictions\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            val_xx = torch.cat((v_pred_mean_list_l1[i][:val_x_h.shape[0],:], val_x_h),-1)\n",
    "            val_predictions = likelihood(model(val_xx))\n",
    "            val_mean = val_predictions.mean.detach().numpy()\n",
    "            val_loss = np.mean(np.abs(val_y_h.numpy() - val_mean))\n",
    "            if val_loss < min_val_loss:\n",
    "                wait = 0\n",
    "                min_val_loss = val_loss\n",
    "                saved_model = model\n",
    "            elif val_loss >= min_val_loss:\n",
    "                wait += 1\n",
    "                if wait == patience:\n",
    "#                     saved_model_list_l2.append(saved_model)\n",
    "#                     test_xx = torch.cat((te_pred_mean_list_l1[i][:test_x_h.shape[0],:], test_x_h),-1)\n",
    "#                     predictions = likelihood(saved_model(test_xx))\n",
    "#                     mean = predictions.mean\n",
    "#                     pred_mean_list_l2.append(mean)\n",
    "                    \n",
    "#                     pred_lower, pred_upper = predictions.confidence_region()\n",
    "#                     pred_std = mean - pred_lower\n",
    "#                     pred_std_list_l2.append(pred_std)\n",
    "                    break\n",
    "        \n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        saved_model_list_l2.append(saved_model)\n",
    "        test_xx = torch.cat((te_pred_mean_list_l1[i][:test_x_h.shape[0],:], test_x_h),-1)\n",
    "        predictions = likelihood(saved_model(test_xx))\n",
    "        mean = predictions.mean\n",
    "        pred_mean_list_l2.append(mean)\n",
    "        \n",
    "        pred_lower, pred_upper = predictions.confidence_region()\n",
    "        pred_std = mean - pred_lower\n",
    "        pred_std_list_l2.append(pred_std)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "380b2190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_metric(pred_mu, pred_std, y):\n",
    "#         pred_mu = torch.from_numpy(pred_mu)\n",
    "#         pred_std = torch.from_numpy(pred_std)\n",
    "#         y = torch.from_numpy(y)\n",
    "        gaussian = torch.distributions.Normal(pred_mu, pred_std)\n",
    "        nll = -gaussian.log_prob(y)\n",
    "        nll = torch.mean(nll).detach().numpy().item()\n",
    "        return nll\n",
    "\n",
    "def mae_metric(y_pred, y_true):\n",
    "    loss = torch.abs(y_pred - y_true)\n",
    "    loss[loss != loss] = 0\n",
    "    loss = loss.mean().detach().numpy().item()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61c10f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7691711627556101, 0.7692180150311272, 0.7674125238387164] [1.7085020014220018, 1.7056576732538988, 1.9208391020433289]\n"
     ]
    }
   ],
   "source": [
    "# MAE\n",
    "# truth = test_y.reshape(-1,6,87,87)\n",
    "truth = test_y_h\n",
    "mae_list = []\n",
    "nll_list = []\n",
    "for i in range(3):\n",
    "    mean = pred_mean_list_l2[i]\n",
    "    std = pred_std_list_l2[i]\n",
    "#     mean = mean.reshape(-1,6,87,87)\n",
    "    mae = mae_metric(mean,truth)\n",
    "    mae_list.append(mae)\n",
    "    nll = nll_metric(mean,std,truth)\n",
    "    nll_list.append(nll)\n",
    "    \n",
    "print(mae_list,nll_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb7c1b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../result/nargp_mae.npy', np.array(mae_list))\n",
    "np.save('../result/nargp_nll.npy', np.array(nll_list))\n",
    "np.save('../result/nargp_mean.npy', torch.stack(pred_mean_list_l2,0).detach().numpy())\n",
    "np.save('../result/nargp_std.npy', torch.stack(pred_std_list_l2,0).detach().numpy())\n",
    "np.save('../result/nargp_truth.npy', test_y_h.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd4ada9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
