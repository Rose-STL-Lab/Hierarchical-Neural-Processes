{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a9172e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e753a48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 45414) (50, 45414) (50, 45414)\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(dataset_dir):\n",
    "    data = {}\n",
    "\n",
    "    for category in ['train_l1', 'train_l2', 'val_l1', 'val_l2', 'test_l1', 'test_l2']:\n",
    "        cat_data = np.load(os.path.join(dataset_dir, category + '.npz'))\n",
    "        data['x_' + category] = cat_data['x']\n",
    "        data['y_' + category] = cat_data['y']\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = load_dataset('mfnp_data/')\n",
    "\n",
    "x_train = data['x_train_l2']\n",
    "x_train = x_train.reshape(x_train.shape[0],-1)\n",
    "y_train = data['y_train_l2']\n",
    "y_train = y_train.reshape(y_train.shape[0],-1)\n",
    "x_val = data['x_val_l2']\n",
    "x_val = x_val.reshape(x_val.shape[0],-1)\n",
    "y_val = data['y_val_l2']\n",
    "y_val = y_val.reshape(y_val.shape[0],-1)\n",
    "x_test = data['x_test_l2']\n",
    "x_test = x_test.reshape(x_test.shape[0],-1)\n",
    "y_test = data['y_test_l2']\n",
    "y_test = y_test.reshape(y_test.shape[0],-1)\n",
    "\n",
    "train_x = torch.from_numpy(x_train)\n",
    "train_y = torch.from_numpy(y_train)\n",
    "\n",
    "val_x = torch.from_numpy(x_val)\n",
    "val_y = torch.from_numpy(y_val)\n",
    "\n",
    "test_x = torch.from_numpy(x_test)\n",
    "test_y = torch.from_numpy(y_test)\n",
    "\n",
    "print(y_train.shape,y_val.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7a517de",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latents = 1\n",
    "num_tasks = 45414\n",
    "\n",
    "class MultitaskGPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self):\n",
    "        # Let's use a different set of inducing points for each latent function\n",
    "        inducing_points = torch.rand(num_latents, 1, 45414)\n",
    "\n",
    "        # We have to mark the CholeskyVariationalDistribution as batch\n",
    "        # so that we learn a variational distribution for each task\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            inducing_points.size(-2), batch_shape=torch.Size([num_latents])\n",
    "        )\n",
    "\n",
    "        # We have to wrap the VariationalStrategy in a LMCVariationalStrategy\n",
    "        # so that the output will be a MultitaskMultivariateNormal rather than a batch output\n",
    "        variational_strategy = gpytorch.variational.LMCVariationalStrategy(\n",
    "            gpytorch.variational.VariationalStrategy(\n",
    "                self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "            ),\n",
    "            num_tasks=45414,\n",
    "            num_latents=1,\n",
    "            latent_dim=-1\n",
    "        )\n",
    "\n",
    "        super().__init__(variational_strategy)\n",
    "\n",
    "        # The mean and covariance modules should be marked as batch\n",
    "        # so we learn a different set of hyperparameters\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([num_latents]))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([num_latents])),\n",
    "            batch_shape=torch.Size([num_latents])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function should be written as if we were dealing with each output\n",
    "        # dimension in batch\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0222819f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2g/wdp07rt57lg1cc7zgtgnr3yh0000gn/T/ipykernel_11502/4255529372.py:34: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  epochs_iter = tqdm.tqdm_notebook(range(num_epochs), desc=\"Epoch\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1971bd33af3b45909d0e8a73c15663e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28df99150494d8cb357256a0eb49927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a4d6f683044c42b927e857145028e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "num_epochs = 2 if smoke_test else 2000\n",
    "\n",
    "random_seeds = [42,43,46]\n",
    "pred_mean_list = []\n",
    "pred_std_list = []\n",
    "saved_model_list = []\n",
    "pred_lower_list = []\n",
    "pred_upper_list = []\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    torch.manual_seed(random_seeds[i])\n",
    "    np.random.seed(random_seeds[i])\n",
    "    random.seed(random_seeds[i])\n",
    "    \n",
    "    model = MultitaskGPModel()\n",
    "    likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=num_tasks)\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.05)\n",
    "\n",
    "    # Our loss object. We're using the VariationalELBO, which essentially just computes the ELBO\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "\n",
    "    # We use more CG iterations here because the preconditioner introduced in the NeurIPS paper seems to be less\n",
    "    # effective for VI.\n",
    "    epochs_iter = tqdm.tqdm_notebook(range(num_epochs), desc=\"Epoch\")\n",
    "\n",
    "    min_val_loss = float('inf')\n",
    "    patience = 100\n",
    "    wait = 0\n",
    "\n",
    "    for j in epochs_iter:\n",
    "        # Within each iteration, we will go over each minibatch of data\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "\n",
    "        epochs_iter.set_postfix(loss=loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # Make predictions\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            val_predictions = likelihood(model(val_x))\n",
    "            val_mean = val_predictions.mean.detach().numpy()\n",
    "            val_loss = np.mean(np.abs(val_y.numpy() - val_mean))\n",
    "            if val_loss < min_val_loss:\n",
    "                wait = 0\n",
    "                min_val_loss = val_loss\n",
    "                saved_model = model\n",
    "            elif val_loss >= min_val_loss:\n",
    "                wait += 1\n",
    "                if wait == patience:\n",
    "#                     saved_model_list.append(saved_model)\n",
    "#                     predictions = likelihood(saved_model(test_x))\n",
    "#                     mean = predictions.mean\n",
    "#                     pred_mean_list.append(mean)\n",
    "#                     pred_lower, pred_upper = predictions.confidence_region()\n",
    "#                     pred_lower_list.append(pred_lower)\n",
    "#                     pred_upper_list.append(pred_upper)\n",
    "#                     pred_std = mean - pred_lower\n",
    "#                     pred_std_list.append(pred_std)\n",
    "                    break\n",
    "        \n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        saved_model_list.append(saved_model)\n",
    "        predictions = likelihood(saved_model(test_x))\n",
    "        mean = predictions.mean\n",
    "        pred_mean_list.append(mean)\n",
    "        pred_lower, pred_upper = predictions.confidence_region()\n",
    "        pred_lower_list.append(pred_lower)\n",
    "        pred_upper_list.append(pred_upper)\n",
    "        pred_std = mean - pred_lower\n",
    "        pred_std_list.append(pred_std)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9951aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_metric(pred_mu, pred_std, y):\n",
    "\n",
    "#         pred_mu = pred_mu * no_y_std + no_y_mean\n",
    "#         pred_std = pred_std * no_y_std\n",
    "#         y = y * no_y_std + no_y_mean\n",
    "        gaussian = torch.distributions.Normal(pred_mu, pred_std)\n",
    "        nll = -gaussian.log_prob(y)\n",
    "        nll = torch.mean(nll).detach().numpy().item()\n",
    "        return nll\n",
    "\n",
    "def mae_metric(y_pred, y_true):\n",
    "#     y_pred = y_pred * no_y_std + no_y_mean\n",
    "#     y_true = y_true * no_y_std + no_y_mean\n",
    "    loss = torch.abs(y_pred - y_true)\n",
    "    loss[loss != loss] = 0\n",
    "    loss = loss.mean().detach().numpy().item()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0828c0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9097384810447693, 0.9097420573234558, 0.909737229347229] [2.2886478900909424, 2.292630910873413, 2.283050537109375]\n"
     ]
    }
   ],
   "source": [
    "# MAE\n",
    "# truth = test_y.reshape(-1,6,87,87)\n",
    "truth = test_y\n",
    "mae_list = []\n",
    "nll_list = []\n",
    "for i in range(3):\n",
    "    mean = pred_mean_list[i]\n",
    "    std = pred_std_list[i]\n",
    "#     mean = mean.reshape(-1,6,87,87)\n",
    "    mae = mae_metric(mean,truth)\n",
    "    mae_list.append(mae)\n",
    "    nll = nll_metric(mean,std,truth)\n",
    "    nll_list.append(nll)\n",
    "    \n",
    "print(mae_list,nll_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad42349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('result/sfgp_mae.npy', np.array(mae_list))\n",
    "np.save('result/sfgp_nll.npy', np.array(nll_list))\n",
    "np.save('result/sfgp_mean.npy', torch.stack(pred_mean_list,0).detach().numpy())\n",
    "np.save('result/sfgp_std.npy',  torch.stack(pred_std_list,0).detach().numpy())\n",
    "np.save('result/sfgp_truth.npy', test_y.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39ae54c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
