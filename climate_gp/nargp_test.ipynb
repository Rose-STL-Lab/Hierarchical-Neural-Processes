{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c2e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a53c1de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87, 1176) (32, 45414) (50, 1176) (50, 45414) (50, 1176) (50, 45414)\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(dataset_dir):\n",
    "    data = {}\n",
    "\n",
    "    for category in ['train_l1', 'train_l2', 'val_l1', 'val_l2', 'test_l1', 'test_l2']:\n",
    "        cat_data = np.load(os.path.join(dataset_dir, category + '.npz'))\n",
    "        data['x_' + category] = cat_data['x']\n",
    "        data['y_' + category] = cat_data['y']\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = load_dataset('nargp_data')\n",
    "\n",
    "x_train_l = data['x_train_l1']\n",
    "x_train_l = x_train_l.reshape(x_train_l.shape[0],-1)\n",
    "y_train_l = data['y_train_l1']\n",
    "y_train_l = y_train_l.reshape(y_train_l.shape[0],-1)\n",
    "x_train_h = data['x_train_l2']\n",
    "x_train_h = x_train_h.reshape(x_train_h.shape[0],-1)\n",
    "y_train_h = data['y_train_l2']\n",
    "y_train_h = y_train_h.reshape(y_train_h.shape[0],-1)\n",
    "\n",
    "x_val_l = data['x_val_l1']\n",
    "x_val_l = x_val_l.reshape(x_val_l.shape[0],-1)\n",
    "y_val_l = data['y_val_l1']\n",
    "y_val_l = y_val_l.reshape(y_val_l.shape[0],-1)\n",
    "x_val_h = data['x_val_l2']\n",
    "x_val_h = x_val_h.reshape(x_val_h.shape[0],-1)\n",
    "y_val_h = data['y_val_l2']\n",
    "y_val_h = y_val_h.reshape(y_val_h.shape[0],-1)\n",
    "\n",
    "x_test_l = data['x_test_l1']\n",
    "x_test_l = x_test_l.reshape(x_test_l.shape[0],-1)\n",
    "y_test_l = data['y_test_l1']\n",
    "y_test_l = y_test_l.reshape(y_test_l.shape[0],-1)\n",
    "x_test_h = data['x_test_l2']\n",
    "x_test_h = x_test_h.reshape(x_test_h.shape[0],-1)\n",
    "y_test_h = data['y_test_l2']\n",
    "y_test_h = y_test_h.reshape(y_test_h.shape[0],-1)\n",
    "\n",
    "train_x_l = torch.from_numpy(x_train_l)\n",
    "train_y_l = torch.from_numpy(y_train_l)\n",
    "\n",
    "train_x_h = torch.from_numpy(x_train_h)\n",
    "train_y_h = torch.from_numpy(y_train_h)\n",
    "\n",
    "val_x_l = torch.from_numpy(x_val_l)\n",
    "val_y_l = torch.from_numpy(y_val_l)\n",
    "\n",
    "val_x_h = torch.from_numpy(x_val_h)\n",
    "val_y_h = torch.from_numpy(y_val_h)\n",
    "\n",
    "test_x_l = torch.from_numpy(x_test_l)\n",
    "test_y_l = torch.from_numpy(y_test_l)\n",
    "\n",
    "test_x_h = torch.from_numpy(x_test_h)\n",
    "test_y_h = torch.from_numpy(y_test_h)\n",
    "\n",
    "print(y_train_l.shape,y_train_h.shape,y_val_l.shape,y_val_h.shape,y_test_l.shape,y_test_h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f6a9d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latents = 1\n",
    "num_tasks_l1 = 1176\n",
    "\n",
    "class MultitaskGPModel_l1(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self):\n",
    "        # Let's use a different set of inducing points for each latent function\n",
    "        inducing_points = torch.rand(num_latents, 1, 1176)\n",
    "\n",
    "        # We have to mark the CholeskyVariationalDistribution as batch\n",
    "        # so that we learn a variational distribution for each task\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            inducing_points.size(-2), batch_shape=torch.Size([num_latents])\n",
    "        )\n",
    "\n",
    "        # We have to wrap the VariationalStrategy in a LMCVariationalStrategy\n",
    "        # so that the output will be a MultitaskMultivariateNormal rather than a batch output\n",
    "        variational_strategy = gpytorch.variational.LMCVariationalStrategy(\n",
    "            gpytorch.variational.VariationalStrategy(\n",
    "                self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "            ),\n",
    "            num_tasks=1176,\n",
    "            num_latents=1,\n",
    "            latent_dim=-1\n",
    "        )\n",
    "\n",
    "        super().__init__(variational_strategy)\n",
    "\n",
    "        # The mean and covariance modules should be marked as batch\n",
    "        # so we learn a different set of hyperparameters\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([num_latents]))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([num_latents])),\n",
    "            batch_shape=torch.Size([num_latents])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function should be written as if we were dealing with each output\n",
    "        # dimension in batch\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5595651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latents = 1\n",
    "num_tasks_l2 = 45414 #45414+1176\n",
    "\n",
    "class MultitaskGPModel_l2(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self):\n",
    "        # Let's use a different set of inducing points for each latent function\n",
    "        inducing_points = torch.rand(num_latents, 1, 46590)\n",
    "\n",
    "        # We have to mark the CholeskyVariationalDistribution as batch\n",
    "        # so that we learn a variational distribution for each task\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            inducing_points.size(-2), batch_shape=torch.Size([num_latents])\n",
    "        )\n",
    "\n",
    "        # We have to wrap the VariationalStrategy in a LMCVariationalStrategy\n",
    "        # so that the output will be a MultitaskMultivariateNormal rather than a batch output\n",
    "        variational_strategy = gpytorch.variational.LMCVariationalStrategy(\n",
    "            gpytorch.variational.VariationalStrategy(\n",
    "                self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "            ),\n",
    "            num_tasks= 45414,\n",
    "            num_latents=1,\n",
    "            latent_dim=-1\n",
    "        )\n",
    "\n",
    "        super().__init__(variational_strategy)\n",
    "\n",
    "        # The mean and covariance modules should be marked as batch\n",
    "        # so we learn a different set of hyperparameters\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([num_latents]))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([num_latents]),active_dims=tuple(range(0,1176)))*\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([num_latents]),active_dims=tuple(range(1176,46590)))+\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([num_latents]),active_dims=tuple(range(1176,46590))),\n",
    "            batch_shape=torch.Size([num_latents])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function should be written as if we were dealing with each output\n",
    "        # dimension in batch\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e4fc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2g/wdp07rt57lg1cc7zgtgnr3yh0000gn/T/ipykernel_11508/2199203907.py:35: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  epochs_iter = tqdm.tqdm_notebook(range(num_epochs), desc=\"Epoch\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a61d4b62a14f399d273c46edfbf949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af49d30b1444d52902d7e668b2644d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e86404f0ca496ea587832225bec739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "num_epochs = 2 if smoke_test else 2000\n",
    "\n",
    "random_seeds = [42,43,46]\n",
    "v_pred_mean_list_l1 = []\n",
    "v_pred_std_list_l1 = []\n",
    "te_pred_mean_list_l1 = []\n",
    "te_pred_std_list_l1 = []\n",
    "t_pred_mean_list_l1 = []\n",
    "saved_model_list_l1 = []\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    torch.manual_seed(random_seeds[i])\n",
    "    np.random.seed(random_seeds[i])\n",
    "    random.seed(random_seeds[i])\n",
    "    \n",
    "    model = MultitaskGPModel_l1()\n",
    "    likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=num_tasks_l1)\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.05)\n",
    "\n",
    "    # Our loss object. We're using the VariationalELBO, which essentially just computes the ELBO\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y_l.size(0))\n",
    "\n",
    "    # We use more CG iterations here because the preconditioner introduced in the NeurIPS paper seems to be less\n",
    "    # effective for VI.\n",
    "    epochs_iter = tqdm.tqdm_notebook(range(num_epochs), desc=\"Epoch\")\n",
    "\n",
    "    min_val_loss = float('inf')\n",
    "    patience = 100\n",
    "    wait = 0\n",
    "\n",
    "    for j in epochs_iter:\n",
    "        # Within each iteration, we will go over each minibatch of data\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x_l)\n",
    "        loss = -mll(output, train_y_l)\n",
    "\n",
    "        epochs_iter.set_postfix(loss=loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # Make predictions\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            val_predictions = likelihood(model(val_x_l))\n",
    "            val_mean = val_predictions.mean.detach().numpy()\n",
    "            val_loss = np.mean(np.abs(val_y_l.numpy() - val_mean))\n",
    "            if val_loss < min_val_loss:\n",
    "                wait = 0\n",
    "                min_val_loss = val_loss\n",
    "                saved_model = model\n",
    "            elif val_loss >= min_val_loss:\n",
    "                wait += 1\n",
    "                if wait == patience:\n",
    "#                     saved_model_list_l1.append(saved_model)\n",
    "#                     te_predictions = likelihood(saved_model(test_x_l))\n",
    "#                     te_mean = te_predictions.mean\n",
    "#                     te_pred_mean_list_l1.append(te_mean)\n",
    "#                     te_pred_lower, te_pred_upper = te_predictions.confidence_region()\n",
    "#                     te_pred_std = te_mean - te_pred_lower\n",
    "#                     te_pred_std_list_l1.append(te_pred_std)\n",
    "                    \n",
    "#                     v_predictions = likelihood(saved_model(val_x_l))\n",
    "#                     v_mean = v_predictions.mean\n",
    "#                     v_pred_mean_list_l1.append(v_mean)\n",
    "#                     v_pred_lower, v_pred_upper = v_predictions.confidence_region()\n",
    "#                     v_pred_std = v_mean - v_pred_lower\n",
    "#                     v_pred_std_list_l1.append(v_pred_std)\n",
    "                    \n",
    "#                     t_predictions = likelihood(saved_model(train_x_l))\n",
    "#                     t_mean = t_predictions.mean\n",
    "#                     t_pred_mean_list_l1.append(t_mean)\n",
    "                    break\n",
    "        \n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        saved_model_list_l1.append(saved_model)\n",
    "        te_predictions = likelihood(saved_model(test_x_l))\n",
    "        te_mean = te_predictions.mean\n",
    "        te_pred_mean_list_l1.append(te_mean)\n",
    "        te_pred_lower, te_pred_upper = te_predictions.confidence_region()\n",
    "        te_pred_std = te_mean - te_pred_lower\n",
    "        te_pred_std_list_l1.append(te_pred_std)\n",
    "\n",
    "        v_predictions = likelihood(saved_model(val_x_l))\n",
    "        v_mean = v_predictions.mean\n",
    "        v_pred_mean_list_l1.append(v_mean)\n",
    "        v_pred_lower, v_pred_upper = v_predictions.confidence_region()\n",
    "        v_pred_std = v_mean - v_pred_lower\n",
    "        v_pred_std_list_l1.append(v_pred_std)\n",
    "\n",
    "        t_predictions = likelihood(saved_model(train_x_l))\n",
    "        t_mean = t_predictions.mean\n",
    "        t_pred_mean_list_l1.append(t_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd546f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2g/wdp07rt57lg1cc7zgtgnr3yh0000gn/T/ipykernel_11508/4034186289.py:32: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  epochs_iter = tqdm.tqdm_notebook(range(num_epochs), desc=\"Epoch\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c52d2663da49628ad361f2f861ad3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58cf62c00684814a8eb15b763ec0da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce6a5ebb4a44668b1ad1ea22046f34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "num_epochs = 2 if smoke_test else 2000\n",
    "\n",
    "random_seeds = [42,43,46]\n",
    "pred_mean_list_l2 = []\n",
    "pred_std_list_l2 = []\n",
    "saved_model_list_l2 = []\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    torch.manual_seed(random_seeds[i])\n",
    "    np.random.seed(random_seeds[i])\n",
    "    random.seed(random_seeds[i])\n",
    "    \n",
    "    model = MultitaskGPModel_l2()\n",
    "    likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=num_tasks_l2)\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.05)\n",
    "\n",
    "    # Our loss object. We're using the VariationalELBO, which essentially just computes the ELBO\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y_h.size(0))\n",
    "\n",
    "    # We use more CG iterations here because the preconditioner introduced in the NeurIPS paper seems to be less\n",
    "    # effective for VI.\n",
    "    epochs_iter = tqdm.tqdm_notebook(range(num_epochs), desc=\"Epoch\")\n",
    "\n",
    "    min_val_loss = float('inf')\n",
    "    patience = 100\n",
    "    wait = 0\n",
    "\n",
    "    for j in epochs_iter:\n",
    "        # Within each iteration, we will go over each minibatch of data\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_xx = torch.cat((t_pred_mean_list_l1[i][:train_x_h.shape[0],:], train_x_h),-1)\n",
    "        output = model(train_xx)\n",
    "        loss = -mll(output, train_y_h)\n",
    "\n",
    "        epochs_iter.set_postfix(loss=loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # Make predictions\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            val_xx = torch.cat((v_pred_mean_list_l1[i][:val_x_h.shape[0],:], val_x_h),-1)\n",
    "            val_predictions = likelihood(model(val_xx))\n",
    "            val_mean = val_predictions.mean.detach().numpy()\n",
    "            val_loss = np.mean(np.abs(val_y_h.numpy() - val_mean))\n",
    "            if val_loss < min_val_loss:\n",
    "                wait = 0\n",
    "                min_val_loss = val_loss\n",
    "                saved_model = model\n",
    "            elif val_loss >= min_val_loss:\n",
    "                wait += 1\n",
    "                if wait == patience:\n",
    "#                     saved_model_list_l2.append(saved_model)\n",
    "#                     test_xx = torch.cat((t_pred_mean_list_l1[i][:test_x_h.shape[0],:], test_x_h),-1)\n",
    "#                     predictions = likelihood(saved_model(test_xx))\n",
    "#                     mean = predictions.mean\n",
    "#                     pred_mean_list_l2.append(mean)\n",
    "                    \n",
    "#                     pred_lower, pred_upper = predictions.confidence_region()\n",
    "#                     pred_std = mean - pred_lower\n",
    "#                     pred_std_list_l2.append(pred_std)\n",
    "                    break\n",
    "        \n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        saved_model_list_l2.append(saved_model)\n",
    "        test_xx = torch.cat((t_pred_mean_list_l1[i][:test_x_h.shape[0],:], test_x_h),-1)\n",
    "        predictions = likelihood(saved_model(test_xx))\n",
    "        mean = predictions.mean\n",
    "        pred_mean_list_l2.append(mean)\n",
    "        \n",
    "        pred_lower, pred_upper = predictions.confidence_region()\n",
    "        pred_std = mean - pred_lower\n",
    "        pred_std_list_l2.append(pred_std)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "380b2190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_metric(pred_mu, pred_std, y):\n",
    "#         pred_mu = torch.from_numpy(pred_mu)\n",
    "#         pred_std = torch.from_numpy(pred_std)\n",
    "#         y = torch.from_numpy(y)\n",
    "        pred_mu = pred_mu.reshape(-1,6,87,87)\n",
    "        pred_std = pred_std.reshape(-1,6,87,87)\n",
    "        y = y.reshape(-1,6,87,87)\n",
    "        nll_list = []\n",
    "        for i in range(6):\n",
    "            gaussian = torch.distributions.Normal(pred_mu[:,i], pred_std[:,i])\n",
    "            nll = -gaussian.log_prob(y[:,i])\n",
    "            nll = torch.mean(nll).detach().numpy().item()\n",
    "            nll_list.append(nll)\n",
    "        return nll_list\n",
    "\n",
    "def mae_metric(y_pred, y_true):\n",
    "    y_pred = y_pred.reshape(-1,6,87,87)\n",
    "    y_true = y_true.reshape(-1,6,87,87)\n",
    "    loss_list = []\n",
    "    for i in range(6):\n",
    "        loss = torch.abs(y_pred[:,i] - y_true[:,i])\n",
    "        loss[loss != loss] = 0\n",
    "        loss = loss.mean().detach().numpy().item()\n",
    "        loss_list.append(loss)\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22c5f804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93210858 0.90160795 0.92718748 0.88728025 0.92905305 0.88155997] [2.30316194 2.30030958 2.30303033 2.29980357 2.30133001 2.29443741]\n"
     ]
    }
   ],
   "source": [
    "# MAE\n",
    "# truth = test_y.reshape(-1,6,87,87)\n",
    "truth = test_y_h\n",
    "mae_list = []\n",
    "nll_list = []\n",
    "for i in range(3):\n",
    "    mean = pred_mean_list_l2[i]\n",
    "    std = pred_std_list_l2[i]\n",
    "#     mean = mean.reshape(-1,6,87,87)\n",
    "    mae = mae_metric(mean,truth)\n",
    "    mae_list.append(mae)\n",
    "    nll = nll_metric(mean,std,truth)\n",
    "    nll_list.append(nll)\n",
    "    \n",
    "nll_arr = np.array(nll_list)\n",
    "mae_arr = np.array(mae_list)\n",
    "\n",
    "nll_mean = np.mean(nll_arr, 0)\n",
    "nll_std = np.std(nll_arr, 0)\n",
    "mae_mean = np.mean(mae_arr, 0)\n",
    "mae_std = np.std(mae_arr, 0)\n",
    "\n",
    "print(mae_mean,nll_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae13cbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9321095943450928, 0.9016100168228149, 0.9271917343139648, 0.8872880935668945, 0.9290631413459778, 0.8815695643424988], [0.9321101903915405, 0.9016080498695374, 0.9271921515464783, 0.8872836828231812, 0.9290578365325928, 0.8815655708312988], [0.9321059584617615, 0.9016057848930359, 0.9271785616874695, 0.8872689604759216, 0.9290381669998169, 0.8815447688102722]] [[2.306328058242798, 2.3038389682769775, 2.3065550327301025, 2.303262710571289, 2.304896831512451, 2.2979745864868164], [2.3079919815063477, 2.3047077655792236, 2.3073601722717285, 2.304313898086548, 2.305748701095581, 2.2991678714752197], [2.295165777206421, 2.292382001876831, 2.295175790786743, 2.2918341159820557, 2.293344497680664, 2.2861697673797607]]\n"
     ]
    }
   ],
   "source": [
    "# MAE\n",
    "# truth = test_y.reshape(-1,6,87,87)\n",
    "truth = test_y_h\n",
    "mae_list = []\n",
    "nll_list = []\n",
    "for i in range(3):\n",
    "    mean = pred_mean_list_l2[i]\n",
    "    std = pred_std_list_l2[i]\n",
    "#     mean = mean.reshape(-1,6,87,87)\n",
    "    mae = mae_metric(mean,truth)\n",
    "    mae_list.append(mae)\n",
    "    nll = nll_metric(mean,std,truth)\n",
    "    nll_list.append(nll)\n",
    "    \n",
    "print(mae_list,nll_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb7c1b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('result/nargp_mae.npy', np.array(mae_list))\n",
    "np.save('result/nargp_nll.npy', np.array(nll_list))\n",
    "np.save('result/nargp_mean.npy', torch.stack(pred_mean_list_l2,0).detach().numpy())\n",
    "np.save('result/nargp_std.npy', torch.stack(pred_std_list_l2,0).detach().numpy())\n",
    "np.save('result/nargp_truth.npy', test_y_h.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ace6a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
